# ğŸ§  Multi-Agent Visual Question Answering System (YOLO + LLM Planner)

## ğŸ“Œ Project Overview

This project is a **multi-agent visual question answering (VQA) system** that combines:

* **YOLO object detection (Ultralytics)** for image understanding
* **A large language model (LLM)** for reasoning and planning
* **A modular agent architecture** where each agent performs a specific vision-related task

The system takes:

* a **natural language question**
* an **image path**

â€¦and automatically determines **which vision agents are required** to answer that question.

---

## ğŸ¯ End Goal of the Project

The ultimate goal of this project is to:

> **Automatically answer image-based questions by dynamically selecting and orchestrating specialized vision agents using an LLM planner.**

Instead of running all computer vision functions blindly, the system:

1. **Analyzes the question**
2. **Selects only the necessary agents**
3. **Executes them to extract the required information**

This architecture is scalable, interpretable, and efficient.

---

## ğŸ§© High-Level Architecture

```
User Question + Image
        â”‚
        â–¼
ğŸ§  Planner Agent (LLM)
        â”‚
        â–¼
Select Required Agents
        â”‚
        â–¼
ğŸ› ï¸ Vision Agents (YOLO-based)
        â”‚
        â–¼
Structured Outputs (counts, locations, image size, visualizations)
```

---

## ğŸ“ Project Structure

```
project/
â”‚
â”œâ”€â”€ DetectionAgent.py        # Object counting
â”œâ”€â”€ LocationAgent.py         # Bounding box locations
â”œâ”€â”€ SizeAgent.py             # Image dimensions
â”œâ”€â”€ VisualizationAgent.py    # YOLO visual output
â”‚
â”œâ”€â”€ ParentAgent.py           # Planner + agent selection logic
â”œâ”€â”€ state.py                 # Shared state definition
â”‚
â”œâ”€â”€ main.py                  # Entry point / example usage
â”œâ”€â”€ images/
â”‚   â””â”€â”€ image2.png
â”‚
â””â”€â”€ README.md
```

---

## ğŸ¤– Vision Agents (YOLO-Based)

All agents use the **Ultralytics YOLO model (`yolo11n.pt`)**.

---

### 1ï¸âƒ£ DetectionAgent â€“ Object Counting

**Purpose:**
Counts how many instances of each object class appear in the image.

**File Logic:**

```python
def getCount(img_path) -> dict
```

**Output Example:**

```json
{
  "person": 3,
  "car": 1,
  "dog": 2
}
```

**Use Case:**

* "How many people are in the image?"
* "Count the objects in this picture."

---

### 2ï¸âƒ£ LocationAgent â€“ Object Locations

**Purpose:**
Returns bounding box coordinates for each detected object.

**File Logic:**

```python
def getLocations(img_path) -> List[str]
```

**Output Example:**

```
Name: person, Location: (x1, y1, x2, y2)
```

**Use Case:**

* "Where are the objects located?"
* "Give bounding box coordinates."

---

### 3ï¸âƒ£ SizeAgent â€“ Image Dimensions

**Purpose:**
Returns the width and height of the input image.

**File Logic:**

```python
def getSize(img_path) -> (width, height)
```

**Use Case:**

* "What is the size of the image?"
* "How large is the image resolution?"

---

### 4ï¸âƒ£ VisualizationAgent â€“ Detection Visualization

**Purpose:**
Displays the YOLO-detected image with bounding boxes.

**File Logic:**

```python
def show_plot(img_path)
```

**Output:**

* Matplotlib window showing detected objects.

**Use Case:**

* Debugging
* Visual confirmation of detections

---

## ğŸ§  Planner Agent (LLM-Based)

### ğŸ” What It Does

The planner uses an **LLM (DeepSeek-R1 via NVIDIA API)** to:

* Read the **user question**
* Decide **which agents are required**
* Return a structured list of agent names

---

### ğŸ“„ Planner Prompt

The planner is instructed using a structured prompt with examples and strict output formatting enforced via **Pydantic**.

**Available Agents:**

* `DetectionAgent`
* `VisualizationAgent`
* `LocationAgent`
* `SizeAgent`

---

### ğŸ§¾ Planner Output Schema

```python
class template(BaseModel):
    requiredAgents: List[str]
```

**Example Output:**

```json
{
  "requiredAgents": ["DetectionAgent"]
}
```

---

## ğŸ“¦ State Management

### `state.py`

Defines the shared state passed across the system:

```python
class state:
    question: str
    image_path: str
    requiredAgents: List[str]
```

This allows:

* Clean separation of concerns
* Easy extension for future agents (e.g., depth, segmentation)

---

## â–¶ï¸ Example Execution Flow

### `main.py`

```python
state["question"] = "How many people are in the image?"
state["image_path"] = "images/image2.png"

result = get_agents(state)
print(result)
```

### What Happens Internally

1. The **planner agent** reads the question
2. It determines that `DetectionAgent` is required
3. The selected agents can then be executed to answer the question

---

## ğŸš€ Key Strengths of This Design

âœ… Modular and extensible agent-based architecture
âœ… Efficient (only required agents are run)
âœ… Interpretable decision-making via LLM planning
âœ… Combines symbolic reasoning with deep learning
âœ… Ideal foundation for advanced VQA systems

---

## ğŸ”® Future Extensions

* Add a **Response Agent** to generate natural language answers
* Integrate **agent execution loop** after planning
* Support **multiple images**
* Add **segmentation or pose estimation agents**
* Persist results using Neptune or other experiment trackers

---

## ğŸ§  Summary

This project demonstrates a **modern AI system design** that fuses:

* **Computer vision**
* **LLM-based reasoning**
* **Multi-agent orchestration**

It is not just detecting objectsâ€”it is **thinking about how to answer questions intelligently**.

If you want, I can next:

* Add agent execution logic
* Convert this into a full VQA pipeline
* Or refactor it into a production-ready framework
